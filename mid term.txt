1. gpgpu: cuda program으로 프로그래밍할 수 있는 gpu


2. cpu의 발달
technology scaling <- 나노공정
improvement in computer architecture <- 어셈블리 없어짐


3. cpu가 잘 발전해서 gpu를 쓰지 않음


4. cpu performance improvement, limitation
increasing clock frequency
instruction-level parallelism(ILP) -> 프로그램을 sequential하게 짰더라도 하드웨어가 자동적으로 parallel하게 실행함
하지만 이 방법으로 더 이상 성능향상 불가능


5. Power = C*V^2*frequency
high power consumption -> high temperature


6. 이제 ILP로 성능 이득이 적다.
한번에 실행할 수 있는 instruction을 늘려도 speedup은 거의 변화가 없다.
independent한 instruction이 4개를 넘지 않는다.


7. types of parallelism
- instruction-level parallelism(ILP)
위의 설명

- Data-Level Parallelism(DLP)
하나의 instruction을 여러 data에 대해 적용한다.
SIMD instruction, GPU

- Tread-level parallelism(TLP)
하나의 task를 여러개 tread로 나눠서 동시에 실행함
Multi-core


8. Flynn's classification
- SISD: single instruction single data stream
pipelining
out-of-order execution
superscalar processor
VLIW processor

- SIMD: '' multiple ''
array processor, vector processor
GPU

- MISD:
신뢰성이 필요한 경우

- MIMD:
shared memory multiprocessor
distributed-memory multiprocessors


9. amdahl's law
multi processor에는 한계가 있다.
Timproved = Taffected/imporvement factor + Tunaffected
0.3T + 0.7T = T
0.3T/2 + 0.7T = 0.85T


10. CPU: Latency oriented design
latency: 하나를 실행하는데 걸리는 시간
large caches
large constrol unit (분기문을 좀 더 빠르게 처리)
powerful alu
sequential


11. GPU: Throughput oriented design
throughput: 정해진 시간에 얼마나 많은 task를 처리했냐
thread pool
small caches
simple control
energy efficient ALUs
parallel


12. excute program
PC resister의 값을 읽고
fetch(decode)해서 읽어오고
ALU에서 연산하고
resister에 저장한다.


13. superscalar processor(ILP사용한다.)
여러개 instruction을 decode한다. 이 때 dependency가 없어야 한다. <- 하드웨어가 찾아줌
resister는 공유한다.


14. out of order control logic -> dependency가 없는 instruction을 먼저 수행해준다.
fundemental limitation이 있다. -> dependency가 없는 instruction이 별로 없다.(4이상 불가능함)


15. multi core
add more, instruction을 동시에 실행한다.
how do you feed all these cores? data-level parallelism


16. SIMD
fetch/decode해오고 ALU가 여러개라면
서로다른 데이터를 이용하여 동일한 연산을 수행한다.

- conditional execution
if condition이 생기면 시간이 더 오래걸릴 수 있다.
if를 먼저 수행하고 나머지는 stall되어있고 else를 실행한다.


17. 
memory latency: 메모리 request에 걸리는 시간, 100 cycles, 100ns
memory bandwidth: 얼마나 많은 데이터를 한번에 읽어올수있나 20GB/s
stall: processor가 instruction을 수행하지 못하고 멈춤


18. hiding stalls with multi-threading
multi thread를 사용하면 stall되면 다른 thread를 돌린다.(memory latency를 hide할 수 있음)
memory bandwidth를 증가시켜야 한다.(latency는 hiding가능하므로)
thread context를 저장할 공간이 필요한다.


19. 
SPA -> TPC-> TEX(graphic을 위한 texture processor), SMs ->(SP: core), SFU(special function unit, 복잡한 연산 수행)
SP는 동일한 program counter를 이용한다.
kernel(GPU code)를 실행하면, 여러개의 thread로 나누어진다.
모든 thread는 동일한 code를 사용한다.

block은 여러개의 thread로 구성, 각 블럭은 SM에 assign된다.
block은 warp단위로 나뉜다. warp = 32 threads(하나의 SM안에 32개의 core가 있기 때문이다.)
동일한 instruction을 동일한 시간에 실행
warp는 concurrently하게 실행된다. 끊기면 다른거 돌리고 이럼

memory access를 할때 latency problem발생
SM hardware implements zero-overhead warp scheduling(context-switching)
-> register file이 많기 때문에
한번에 평균 4instruction을 수행하고 100cycle이 걸리는 load를 한다면 26개의 warp가 필요함


20. CUDA이전에는
data를 image로 바꿔줘야했고, 배우는 데 너무 오래 걸렸다.
graphics API overhead도 컸다.


21. CUDA(compute unified device architecture)
CUDA libraries
CUDA runtime
CUDA driver
를 이용하여 코딩이 가능하다.
범용적으로 사용가능해졌다.

-feature
heterogenous - mixed serial-parallel programming(CPU GPU둘다 사용)
scalable - hierarchical thread execution model
accessible - minimal but expressive changes to C(C언어로 사용가능)


22.
Host: CPU + main memory
Device: GPU + video memory
GNU gcc: linux c compiler
nvcc: NVIDIA CUDA compiler
->
Host code는 GNU gcc로 컴파일 되고
Device code는 nvcc로 컴파일 된다.
두개의 실행파일은 하나의 실행파일로 묶인다.


23. CUDA program uses CUDA memory
GPU cores share
global memory -> DRAM

CPU and GPU have separate memory spaces
allocate -> transfer(C->G) -> transfer(G->C) -> release

- cuda malloc(gpu memory할당), 에러코드를 리턴, 성공하면 cudaSuccess리턴
cudaMalloc( (void**)&dev_a,  nbytes);
cudaMemset( dev_a, 0, nbytes);
cudaFree(dev_a);

if (cudaMalloc(&devPtr, SIZE) != cudaSuccess) {exit(1);}

- data copy
CPU는 복사가 끝날때까지 멈춘다.
GPU에서 어떤 일을 실행하고 있으면 다 끝날때까지 기다린다.
cudaError_t cudaMemcpy( void* dst,   void* src,  size_t nbytes, enum cudaMemcpyKind direction);
2번째 인자에서 1번째 인자로
4번째 인자 예시 cudaMemcpyDeviceToHost


24. cuda error
const char* cudaGetErrorName(cudaError_t err) -> 에러를 string으로 고대로 return해줌
cudaGetErrorString -> 에러를 사람이 알게 쉽게 string으로 바꿔준다.
cudaGetLastError -> 마지막 에러 코드를 return해주고, cudaSuccess로 바꿔준다.
cudaPeekAtLastError -> 마지막 에러 코드를 return해주고, cudaSuccess로 바꿔준다.


25. data parallelism
image processing
molecular dynamics
airline scheduling
starcraft


26. CUDA function declarations
compilation unit: functions(function단위로 compile한다.)
__host__:적지 않아도됨 CPU에서 불림
__device__: GPU에서 호출되고 실행될 수 있는 코드이다.
__global__: CPU에서 호출이되고, GPU에서 실행이된다. 반드시 void를 return해준다.
__device__와 __host__는 같이 사용할 수 있다.(host, device실행파일을 만들어준다.)

__global__
void addKernel(int* A_d, int* B_d, int* C_d)
{
	int i = threadIdx.x; <- Built-in variable
	C_d[i] = A_d[i]+B_d[i];
}


27. cuda kernel function and threading
same kernel function on different cores in parallel
SPMD: Single Program Multiple Data
다른 SM에서 실행

same instructions on different cores
SIMT: Single Instruction Multiple Thread
같은 SM에서 실행

Grid -> block -> thread (직접 크기 지정)


28. kernel launch
addKernel<<<1, SIZE>>>(A_d, B_d, C_d);
kernel name, grid크기, block크기, parameter들


29. process
프로그램을 실행했을 때 나오는 instance
program code+execution state


30. thread
a flow of execution within a process
execution state(address of the next instruction, register state, stack)


31. thread execution
on single core precessors
single thread -> serial processing
multiple thread -> time sharing
on multicore processors
single thread, multiple process -> parallel processing
multiple thread -> parallel processing


32. hierarchy of threads
block안의 thread는 shared memory로 공유할 수있다. 
하나의 block은 하나의 SM에 할당이 된다.
real-world: video -> image


33. Dimensions
threadIdx: unique within a block
blockIdx: unique within a grid
blockDim: dimension of block
gridDim: dimension of grid

dim3 DimGrid(5,1,1); <- dim3로  DimGrid object를 만든다.
dim3 DimBlock(128,1,1);

const dim3 dimGrid(ceil(cols/16.0), ceil(rows/16.0),1);
const dim3 dimBlock(16,16);

int ix = blockIdx.x*blockDim.x + threadIdx.x;


34. 유동적인 grid크기 생성
addKernel<<<ceil(n/256.0), 256>>>(A_d, B_d, C_d,n);
function내에서
if(i<n) C_d[i] = A_d[i] + B_d[i]; 를 이용해야함

thread block의 실행 순서는 알 수 없다.

scalable parallel execution
-> gpu성능에 따라 실행 시간이 다르다.
-> 동일한 프로그램을 모든 gpu에서 사용할 수 있다.
(block단위로 SM에서 실행하게 하여 이를 보장한다.)
64~128 block을 parallel하게 사용가능할 수 있다.


35. 2차원이어도 1차원으로펴져서 저장된다.


36. synchronization
race condition이 일어나지 않도록 조심해야한다.
__syncthreads();를 활용하여 모든 thread의 시간을 맞춘다.


37. transparent scalability
동일한 프로그램을 SM개수가 달라도 실행이 가능하게 만든다.


38. 
SM은 hardware resources(resisters)를 저장할 수 있는 개수가 제한적이기 때문에
block과 thread의 개수의 할당이 제한적이다.

cuda runtime에 실행이 끝난 블럭을 실행해야 하는 새로운 블럭으로 바꾸어 준다.

thread에 할당된 블럭이 작다면, SM에 할당되는 total number of thread
가 감소하여 low occupancy를 갖는다.
ex) 32block이 SM에 할당될수 있고, block은 8개 thread, 
SM은 2048개의 thread가 할당될수 있다면 자원이 낭비된다.


39. how to find out the amount of resources
GPU에 있는SM의 개수
SM에 할당할 수 있는 block, thread의 개수

int dev_count
cudaGetDeviceCount(&dev_count); <- device의 개수
cudaDeviceProp dev_prop;
cudaGetDeviceProperties(&dev_prop, i); <- 특정device의 정보

- cudaDeviceProp
warpSize
totalGlobalMem
maxThreadsPerBlock
multiProcessorCount: SM의 개수
clockRate
maxThreadsDim[3]: maximum number of threads allowed along each dimension of a block
MaxGridSize[3]: number of blocks allowed along each dimension of a grid

SM에서 최대의 thread를 사용할 수 있도록 코드를 짜줘야 한다.


40. thread scheduling and latency tolerance 
block들은 sm에 할당되고, block은 warp로 나뉘어서 실행된다.

round robin 방식을 이용하여 스케줄링해준다.
scoreboarding을 이용하여 hazard(모든 warp가 stall되는 상황)를 막아야 한다.

- scoreboarding
하나의 thread가 실행상태가 되는지 확인해준다.
table형태의 structure를 넣게 된다.
instruction buffer에 있는 모든 instruction에 대해서 operand가 resister에 있는지 모니터링해준다.
모든 값이 resister에 준비되면 ready상태가 된다.


41. 1개의 block을 사용하면,
SM이 담을 수 있는 thread의 개수의 제한이 있다.
global memory access를 많이 한다.


42. tile multiplication
dim3  dimGrid( ceil(WIDTH / TILE_WIDTH), ceil(WIDTH / TILE_WIDTH), 1 );
dim3  dimBlock( TILE_WIDTH, TILE_WIDTH, 1 );
y = blockIdx.y* blockDim.y+ threadIdx.y;
x = blockIdx.x* blockDim.x+ threadIdx.x;


43. resisters -> CPU cache -> DRAM -> flash drive -> tape drive
왜 memory hierarchy가 잘 동작하나? -> Locality
Temporal locality: 최근 access 한 address는 다시 접근한다.
Spatial locality: 근처의 data가 다시 접근될 확률이 높다.


44.
memory를 접근하는데 오랜 시간이 걸린다.
global memory는 유한한 bandwidth를 갖는다.

- Compute-to-global-memory-access-ratio
high compute-to-global-memory-access-ratio -> high performance
global memory access당 얼마나 많은 연산을 수행하였는가

ex)1000GB/s -> 250 giga single-precision numbers per second
2번의 global memory 접근, 2번의 연산
compute-to-global-memory-access-ratio = 1
sum += a[y*width+k] * b[k*width+x];

즉 250GFLOP -> giga floating point operations per second
12TFLOP가 되려면 compute-to-global-memory-access-ratio가 48이 되어야 한다.


45. cuda memories
- per thread resisters: 1cycles
- per block shared memory: ~5cycles <- inter-thread communication
- per grid global memory: ~500 cycles <- inter-block communication
- per thread local memory: ~500 cycles
- per grid constant memory: ~5 cycles(읽기만 가능)

device memory(global memory, local memory, constant memory)


46. shared memory
- explicitly declared되어 사용된다.
- latency, throughput이 우수하다.
- shared memory는 block들에 의해 나뉘어서 사용된다.
- shared memory와 L1캐쉬는 물리적으로 동일(same hardware)한데 나눠서 사용한다.


47. resisters
- fastest memory
- private per thread
- resister는 thread별로 나뉘어서 값을 저장한다.


48. local memory
resister의 값이 너무 많으면 local memory에 저장하게 된다.
private per thread
resister spilling -> resister값이 너무 많아 local memory에 카피한다.


49. global memory
모든 thread가 공유한다.
gpu core 밖에 있다
host와 gpu가 모두 접근 가능하다.


50. constant memory
shared by all thread	
read only data
off-chip device memory
host와 gpu가 모두 접근 가능하다.
host가 constant값을 써줘야 gpu에서 읽을 수 있다.
resister보다 느리다.


51. L1/L2 cache(hardware-managed)
global 과 SM사이에 L2 cache(local, global memory저장)
shared memory, L1 cache(사이즈 설정 가능, 증가하면, shared memory감소)


52. variable type
int var -> resister, 많으면 local로
int array_var[10] -> local
__shared__ int shared_var; -> shared
__device__ int global_var; -> global
__constant__ int constant_var -> constant

host에서 사용해야한다 -> constant, device
아니다 -> shared, int, int array_var[10];

shared, global -> race condition발생가능성있음
barriers를 만든다.
__syncthreads()를 사용한다. -> 너무 자주사용하면 안된다.


53. global memory를 많이 접근하면 shared_memory를 사용한다.
i 번째 - i-1번째할때
global -> 2N(global load)
improved -> N + N/BLOCK_SIZE(global load)

shared memory 크기를 알 수 없을 때 extern으로 선언
extern __shared__ int s_data[];
adj_diff<<<num_blocks, block_size, block_size* sizeof(int)>>>(r,i)
가운데 3번째 인자로 크기를 지정해준다.


54.
tiled matrix multiplication -> global memory접근이 너무 많다!
input, output matrix를 tile로 모두 나누고
shared memory로 tile을 올려줘서 부분합을 구하고 차례대로 더한다.

bx = blockIdx.x,
by = blockIdx.y
tx= threadIdx.x
ty = threadIdx.y
gy= by * TILE_WIDTH + ty
gx= bx * TILE_WIDTH + tx
A[gy][m*TILE_WIDTH+tx]를 shared memory에 올린다.
B[m*TILE_WIDTH+ty][gx]를 shared memory에 올린다.
m < ceil(width / (float)TILE_WIDTH)
s_A[ty][tx] = g_A[gy* width + (m * TILE_WIDTH + tx)];
s_B[ty][tx] = g_B[(m * TILE_WIDTH + ty) * width + gx];

__syncthreads(); <- load다할때까지, 계산다 끝날때까지, 2개필요하다.

tile width가 16일 경우
2*256 = 512 global memory access
256*(2*16: 한 곳에서 실행되는 연산의 수) = 8192 op
8192 op/512 load = 16 op/load

tile width가 32일 경우
2*1024 = 2048 global memory access
1024*(2*32) = 8192 op
65536 op/2048 load = 32 op/load

- shared memory가 48KB일 경우
tile_width = 16일 경우
2*256*4B = 2KB의 shared memory를 사용한다.
-> 24 active thread block을 실행할 수 있다.
24 * 2 * 256 = 12K pending load(크면 hiding가능)를 가진다.
(active thread block * global memory 접근 * thread per block)


55. memory resources as limit to parallelism
resources are finite
- fewer threads 를 SM이 수용하게 되면
concurrent하게 돌아가는 thread가 감소하고
warp의 개수가 감소하여 scheduling이 가능한 warp의 수가 감소한다.


56. global memory
dram으로 구현된다. -> 높은 bandwidth
bandwidth를 잘 이용해야한다.

channel -> 독립적으로 존재하고, 통로이다. 2개의 memory chip에 연결되어있다.
	한번에 64bit를 전송 가능하다
	channel-level parallelism을 잘 이용해야한다.

chip -> bank, d-mux(bank 몇인지 구해줌)로 구성됨
bank -> command address data bus를 공유한다.
	bank는 독립적으로 동작한다. 한채널에 여러개 request를 보내서 한번에 처리 가능하다.
	depend하면,차가 다 빠져나가고 차가 들어올 수 있다.
	bank-level parallelism

breaking down a bank -> bank는 row * column으로 이루어진다. tile하나에 버스폭만큼저장 가능하다.
	DRAM row = DRAM page
	row decoder: address중 MSB~중간까지 row decoder에 넣는다.
	row buffer(sense amplifier): row decoder에서 읽혀 row가 저장된다.
	column mux: row buffer에서 몇번째 데이터를 보내줄건지 결정한다.( 중간~LSB )

	DRAM cell이 느리기 때문에 하나 읽는데 시간이 너무 오래걸린다.
	그래서 하나의 row를 모두 읽어서 row buffer에 넣고 빠르게 읽어가게 한다.

access to an close row(bank CONFLICT) <- row buffer에 data가 없는 경우
activate: row buffer로 data를 옮긴다.
read/write: row buffer에서 읽고 씀
precharge: row buffer의 값을 지우고 다음값이 들어올수 있도록 준비한다.
activate precharge 매우느림

row buffer hit를 늘려야 한다.
accesses on consecutive memory address -> high row buffer hit rate
sequential하게 access하면된다.


57. DRAM Bursting
block단위로 전송하게 된다.
필요한 데이터가 하나이더라도 데이터가 포함된 block전체를 보낸다.
한개씩 보내는게 아니라 한번에 block단위로 보내서 시간내에 더 많은 데이터가 이동 가능하다.


58. Coalescing memory requests(합체하다)
차에 여러명이 타고 간다.
연속된 메모리를 접근하면 좋다. <- 최대 bandwidth사용가능
align되어있으면 좋다.(32, 64, 128 bytes로)

uncached되면 32B
cached되면 128B (L2 cache사용)

shared memory사용하지않을 경우 mat mul은 coalescing하지 않다.
shared memory를 사용할 경우
tile_width가 32인 경우, shared memory를 접근할 때
tx가 바뀌는데 이때 항상 sequential하게 접근한다.(warp내에서)
coalescing하다


59. cudaMalloc은 256배수의 boundary를 가지도록 메모리를 할당해준다.
row를 128B로 align되지 않으면 coalescing에 좀더 느려진다.

이를 해결해주기 위해 함수를 사용한다.
cudaMallocPitch((void **)devPtr, pitch, width, height);
pitch: 패딩+width

cudaMalloc3D(struct cudaPitchedPtr* pitchedDevPtr,
	struct cudaExtent extent);

cudaExtentextent = make_cudaExtent(width *sizeof(float), height, dept);
cudaPitchedPtr devPitchedPtr;
cudaMalloc3D(&devPitchedPtr, extent)

device code에서 devPitchedPtr.ptr같은 값을 사용하여 접근한다.

cudaMemcpy2D(void* dst, size_t dpitch, const void* src, size_t spitch, size_t width, size_t height, enum cudaMemcpyKind kind);
cudaMemcpy3D(const struct cudaMemcpy3D Params* p);
이용하여 padding을 포함한 값을 copy해준다.


60. shared memory organization
shared memory is divided into banks
-> bandwidth를 높이기 위해서

각 thread들은 동일한 warp에 대해 다른 bank에 접근하여 사용할 수 있게 된다.


61. bank memory layout
bank number = address % 32

__shared__ float a[1024];
bank 0 -> a[0], a[32]
bank 1 -> a[1], a[33]

- broadcast: 여러개의 thread가 같은 메모리를 접근할 때 a[32]를 동시에 접근
single clock cycle이 걸린다.
- multicast: 여러개의 thread가 서로 다른 bank를 접근하지만 같은 값을 접근한다.
single clock cycle이 걸린다.
- serialization case: 각 thread가 a[0], a[32], a[64]...로 접근할 때, 같은 bank에서 값을 읽을 때
bank conflict! 32 thread -> 32 cycles(32 way bank conflicts)


62. avoid bank conflict
memory padding해주면 한칸씩 밀리면서 다른 bank에 접근하게 된다.

tiled matrix multiplication -> no bank conflict
tile width = 32일 때 
s_A[ty][k]에서 같은 value를 broadcasting해준다.
tile width = 16일 때
s_A[ty][k]에서 ty가 1, 0이 가능하게 된다.
두 값을 multicasting을 해준다.


63. control flow divergence
warp들이 different control flow path를 갈때 느려진다.
if-else, for
different execution paths within a warp can be serialized
warp내에서 execution path가 다를 경우 문제가 생긴다.

만약 branch granularity(분기하는 단위) < warp size
different execution paths within a warp are serialized
if (threadIdx.x/ WARP_SIZE > 2) 이런식이 이상적이다.

divergent iteration -> iteration 횟수가 각자 다른 경우
작은건 빨리 끝나고, 큰건 느리게 끝난다.

result[gx/ 2] = input[gx]; <- 짝수인 thread
result[HALF + gx/ 2] = input[gx]; <- 홀수인 thread
를 이렇게 고쳐야함
result[gx] = input[gx* 2];
result[gx] = input[(gx–HALF) * 2 + 1]


64. Occupancy
현재 등록이 된 warp(active warp:실행 준비된 warp)/SM에 등록이 가능한 warp의 최대 개수
shared memory는 block별로, resister는 thread별로 할당된다.

- resister usage per thread
- shared memory per thread block
- block size

resource limit
resister나 shared memory가 다 차면 더 이상 할당해줄 수 없다.

thread가 중복적인 일을 하면, 더 적은 thread로 더 많은 일을 시키는게 좋을 수도 있다.
tiled에서 중복되게 shared memory에 올려주는 경우가 있다. -> 하나의 thread block으로 합친다.

























